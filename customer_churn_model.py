# -*- coding: utf-8 -*-
"""customer_churn_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fLuXqysXg58C8mhMv9EaD1q7foqQRWqD
"""

# for numerical computing
import numpy as np

# for dataframes
import pandas as pd

# for easier visualization
import seaborn as sns

# for visualization and to display plots
from matplotlib import pyplot as plt

# import color maps
from matplotlib.colors import ListedColormap

# Ignore Warnings
import warnings
warnings.filterwarnings("ignore")

from math import sqrt

# to split train and test set
from sklearn.model_selection import train_test_split

#importing the required ML libraries
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import sklearn.metrics as metrics

df = pd.read_csv(r"customer_data.csv")
df.head()

df.drop('customer_id', axis='columns', inplace=True)
df.dtypes

df.drop('year', axis='columns', inplace=True)

df.info()

#printing out a list of all the columns in our training dataset
df.columns

"""Types of features :

1) Categorical : gender, multi_screen, mail_subscribed, churn
2) Continuous : age, no_of_days_subscribed, weekly_mins_watched, minimum_daily_mins, maximum_daily_mins, weekly_max_night_mins, videos_watched, maximum_days_inactive, customer_support_calls
3) Alphanumeric: phone_no
"""

df.drop('phone_no', axis='columns', inplace=True)

df.head()

#filling the missing Embarked values in train and test datasets
df.gender.fillna('Female',inplace=True)

df.isnull().sum()

print("The Median is :", int(df.maximum_days_inactive.median()))

#filling the missing values in the Maximum inactive days column
df.maximum_days_inactive.fillna(3, inplace=True)

print("The Median is :", int(df.churn.median()))

"""## Filling the missing values in the Churn column"""

#filling the missing values in the Churn column
df.churn.fillna(0, inplace=True)

df.isnull().sum()

"""## Converting categorical variables into numerical ones"""

#Converting categorical variables into numerical ones
df2 = pd.get_dummies(df,columns=['gender','multi_screen','mail_subscribed'],drop_first=True)
df2.head()

"""## Splitting out trianing data into X: features and Y: target"""

#Splitting out training data into X: features and Y: target
X = df2.drop("churn",axis=1)
Y = df2["churn"]

### Sandardization of data ###
### We do not standardize the Target variable for classification
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
# Storing the fit object for later reference
sc=sc.fit(X)
# Generating the standardized values of X and y
X=sc.transform(X)


#splitting our training data again in train and test data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=42)

"""## Logistic Regression"""

#Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train,Y_train)
Y_pred = logreg.predict(X_test)
acc_logreg = round(accuracy_score(Y_pred, Y_test) * 100, 2)
acc_logreg

#let's perform some K-fold cross validation for logistic Regression
cv_scores = cross_val_score(logreg,X,Y,cv=5)

np.mean(cv_scores)*100

accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

print(classification_report(Y_test, Y_pred))

"""## Decision Tree Classifier"""

#Decision Tree Classifier

decisiontree = DecisionTreeClassifier()
dep = np.arange(1,10)
param_grid = {'max_depth' : dep}

clf_cv = GridSearchCV(decisiontree, param_grid=param_grid, cv=5)

clf_cv.fit(X, Y)
clf_cv.best_params_,clf_cv.best_score_*100
print('Best value of max_depth:',clf_cv.best_params_)
print('Best score:',clf_cv.best_score_*100)

classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, Y_train)
Y_pred=classifier.predict(X_test)

accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

print(classification_report(Y_test, Y_pred))

"""## Random Forest Classifier"""

#Random Forest Classifier

random_forest = RandomForestClassifier()
ne = np.arange(1,20)
param_grid = {'n_estimators' : ne}

rf_cv = GridSearchCV(random_forest, param_grid=param_grid, cv=5)

rf_cv.fit(X, Y)
print('Best value of n_estimators:',rf_cv.best_params_)
print('Best score:',rf_cv.best_score_*100)

classifier = RandomForestClassifier(n_estimators = 11, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, Y_train)
Y_pred=classifier.predict(X_test)

accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred))

"""##Gradient Boosting Classifier"""

#Gradient Boosting Classifier

gbk = GradientBoostingClassifier()
ne = np.arange(1,20)
dep = np.arange(1,10)
param_grid = {'n_estimators' : ne,'max_depth' : dep}

gbk_cv = GridSearchCV(gbk, param_grid=param_grid, cv=5)

gbk_cv.fit(X, Y)
print('Best value of parameters:',gbk_cv.best_params_)
print('Best score:',gbk_cv.best_score_*100)

classifier=GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, Y_train)
Y_pred=classifier.predict(X_test)

accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

print(classification_report(Y_test, Y_pred))

"""## KNN classificattion"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)

knn.fit(X_train, Y_train)
Y_pred = knn.predict(X_test)
acc_knn = round(accuracy_score(Y_pred, Y_test) * 100, 2)
acc_knn

accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

print(classification_report(Y_test, Y_pred))

"""## Gradient boost on test data"""

classifier = GradientBoostingClassifier(n_estimators =13, max_depth = 5)
classifier.fit(X_train, Y_train)
classifier.score(X_test, Y_test)

Y_pred = classifier.predict(X_test)

accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

print(classification_report(Y_test, Y_pred))

"""## XGBOOST"""

from xgboost import XGBClassifier
classifier = XGBClassifier()
classifier.fit(X_train, Y_train)

y_pred = classifier.predict(X_test)
accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train, y = Y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

print(classification_report(Y_test, y_pred))

"""## Support vector machine"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, Y_train)
y_pred = classifier.predict(X_test)

accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

print(classification_report(Y_test, y_pred))

"""## Kernel SVM"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, Y_train)

y_pred = classifier.predict(X_test)
accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

print(classification_report(Y_test, y_pred))

"""## Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, Y_train)

y_pred = classifier.predict(X_test)

accuracy = metrics.accuracy_score(Y_test,Y_pred)
confusion_matrix = metrics.confusion_matrix(Y_test,Y_pred)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

print(classification_report(Y_test, y_pred))

"""

```
# This is formatted as code
```

## Lazyboost and light gradient boosting method"""

!pip install lazypredict

import lazypredict
from lazypredict.Supervised import LazyClassifier

import lightgbm as lgb

clf = LazyClassifier(verbose=0, ignore_warnings=False, custom_metric=None)
models, predictions = clf.fit(X_train, X_test, Y_train, Y_test)
model_dictionary = clf.provide_models(X_train, X_test, Y_train, Y_test)
models

model = lgb.LGBMClassifier(verbose=0)
model.fit(X_train,Y_train)

y_test_predicted = model.predict(X_test)

from sklearn import metrics

accuracy = metrics.accuracy_score(Y_test,y_test_predicted)
confusion_matrix = metrics.confusion_matrix(Y_test,y_test_predicted)

print('The accuracy score is ',accuracy*100,'%')
sns.heatmap(confusion_matrix)
print(confusion_matrix)

print(classification_report(Y_test, y_pred))